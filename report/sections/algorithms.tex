\chapter{Algorithms}

In general, graph-based ANNS algorithms are separated into two main phases.
\begin{enumerate}
    \item Index construction
    \item Search 
\end{enumerate}
The search phase is very similar across all algorithms. In short, it is simply a \textsc{GreedySearch} (Algorithm \ref{alg:greedy-search}) algorithm that is modified to fit the specific graph structure. The main difference between algorithms lies in the index construction step---how the graph is constructed and how the vertices inside it are connected.

\section{Search}

\begin{algorithm}[H]
    \caption{\textsc{GreedySearch}(Query \(q\), Starting \(s\), Max candidate size \(L\))}\label{alg:greedy-search}
    \begin{algorithmic}[1]
        \State{\(\mathcal{L} \gets \left\{s\right\}\); \(\mathcal{V} \gets \emptyset\)} \Comment{candidates \(\mathcal{L}\), visited \(\mathcal{V}\)}
        \While{\(\mathcal{L} \setminus \mathcal{V} \neq \emptyset\)}
            \State{\(p^* \gets \arg\min_{p \in \mathcal{L} \setminus \mathcal{V}} \delta(p, q)\)}
            \State{\(\mathcal{L} \gets \mathcal{L} \cup N_{\text{out}}(p^*)\)}
            \State{\(\mathcal{V} \gets \mathcal{V} \cup \left\{p^*\right\}\)}
            \If{\(|\mathcal{L}| > L\)}
                \State{\(\mathcal{L} \gets \textsc{GetClosest}(\mathcal{L}, L)\)}
            \EndIf
        \EndWhile
        \State{\Return{\(\mathcal{L}, \mathcal{V}\)}}
        % \State{Init \(\mathcal{L}\) as work queue with \(s\)}
        % \State{Init \(\mathcal{V}\) to store visited vertices}
        % \While{\(\mathcal{L}\) contains unvisited vertices}
        %     \State{Let \(p^*\) be point closest to \(q\) in \(\mathcal{L}\)}
        %     \State{Add \(N(p^*)\) to \(\mathcal{L}\)}
        %     \State{Mark \(p^*\) as visited}
        %     \If{\(|\mathcal{L}| > L\)}
        %         \State{Pick \(L\) points closest to \(q\) in \(\mathcal{L}\) to keep}
        %     \EndIf
        % \EndWhile
        % \State{\Return{\([\mathcal{L}, \mathcal{V}]\)}}
    \end{algorithmic}
\end{algorithm}

Intuitively, \textsc{GreedySearch} searches for the query \(q\) by first storing a collection of nearest neighbor candidates \(\mathcal{L}\) and a collection of visited vertices \(\mathcal{V}\). Starting from some starting point \(s\) (usually determined by the graph), it greedily picks its neighbor that is closest to \(q\) and traverses the graph until it cannot anymore. A visualization of this can be see in Figure \ref{fig:greedy-viz}.

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{images/greedy-search-begin}
        \caption{Input}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{images/greedy-search-1}
        \caption{Iteration 1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{images/greedy-search-2}
        \caption{Iteration 2}
    \end{subfigure}
    \hfill

    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{images/greedy-search-3}
        \caption{Iteration 3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{images/greedy-search-4}
        \caption{Iteration 4}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\textwidth]{images/greedy-search-final}
        \caption{Returning values}
    \end{subfigure}
    \hfill
    \caption{Visualization of \textsc{GreedySearch}}
    \label{fig:greedy-viz}
\end{figure}

In addition, we can see in Figure \ref{fig:greedy-viz} that the collection of visited vertices \(\mathcal{V}\) can be thought of as the path from the starting point \(s\) to the query point \(q\) as well.

\section{Index Construction}

Similar to the search phase, the index construction phase have similar flows across algorithms as well. The general flow can be seen in Algorithm \ref{alg:build-index}.

\begin{algorithm}[H]
    \caption{\textsc{BuildIndex}(Data \(\mathcal{P}\), Start point \(s\), Beam width \(L\), Degree bound \(M\))}
    \label{alg:build-index}
    \begin{algorithmic}[1]
        \For{\(p \in \mathcal{P}\)}
            \State{Add \(p\) to graph}
            \State{\([\mathcal{L}, \mathcal{V}] \gets \textsc{GreedySearch}(p, s, L, 1)\)} \Comment{Searches for \(p\) in existing graph}
            \State{\(N(p) \gets \textsc{Prune}(p, \mathcal{V})\)}
        \EndFor
    \end{algorithmic}
\end{algorithm}

Intuitively, the algorithm adds the point \(p\) into the graph as a vertex, runs a search to \(p\), and assigns the \textit{path} from the starting point \(s\) to \(p\) after it has been pruned using some heuristics determined by the graph.

\section{HNSW and DiskANN}

Among the existing ANNS algorithms, there are two outstanding algorithms: Hierarchical Navigable Small World (HNSW) and DiskANN. Since vertex removal should not degrade the performance and quality of the graph, it is better to design the removal algorithm on top of an existing algorithm instead of starting from scratch.

\begin{figure}[ht]
    \centering
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=0.2\textheight]{images/hnsw}
        \caption{Hierarchical Navigable Small World (HNSW)}
        \label{fig:hnsw-viz}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=0.2\textheight]{images/diskann}
        \caption{DiskANN}
        \label{fig:diskann-viz}
    \end{subfigure}
    \hfill
    \caption{Visualization of existing, best-performing algorithms}
\end{figure}

\subsection{Hierarchical Navigable Small World (HNSW)}

As shown in Figure \ref{fig:hnsw-viz}, HNSW is a multi-layer graph where the points are \textit{promoted} and cloned to higher layers based on exponentially decaying probability. That is, the probability makes it so that less and less points show up in higher layer.

HNSW constructs its graphs incrementally by adding the points to the graph one-by-one. The construction first decides which layer the new point belongs to and run Algorithm \ref{alg:build-index} on each layer top to bottom. As for its pruning criteria, it ensures that there is a small, circular radius around the new point. The search is done on every layer but the \(k\) nearest neighbors are only picked from the lowest layer.

% \begin{algorithm}[H]
% \caption{SelectNeighbors Algorithm}\label{alg:select-nbrs}
% \begin{algorithmic}[1]
%     \Function{SelectNeighbors}{\(G, q, \mathcal{C}, M\){}}
%         \State{\(\mathcal{R} \gets \emptyset\); \(\mathcal{W} \gets \mathcal{C}\)} \Comment{keepers \(\mathcal{R}\), candidates \(\mathcal{W}\)}
%         \While{\(|\mathcal{W}| > 0\) and \(|\mathcal{R}| < M\)}
%             \State{\(p^* \gets \arg\min_{p \in \mathcal{W}} \delta(p, q)\)}
%             \If{\(\delta(p^*, q) < \min_{p' \in \mathcal{R}} \delta(p', q)\)}
%                 \State{\(\mathcal{R} \gets \mathcal{R} \cup \left\{e\right\}\)}
%             \EndIf
%         \EndWhile
%         \State{\Return{\(\mathcal{R}\)}}
%     \EndFunction
% \end{algorithmic}
% \end{algorithm}

% \textbf{Inputs:} Graph \(G\), Query \(q\), Candidates \(\mathcal{C}\), Max degree \(M\)



% \begin{algorithm}[H]
% \caption{HNSW Algorithm}\label{alg:hnsw}
% \begin{algorithmic}[1]
%     \Function{HNSW}{\(\mathcal{P}, C, M, m_L\)}
%         \State{\(G \gets\) empty graph}
%         \For{\(p \in \mathcal{P}\)}
%             \State{\(\mathcal{W} \gets \emptyset\), \(ep \gets s\)}
%         \EndFor
%         \State{\Return{\(G\)}}
%     \EndFunction
% \end{algorithmic}
% \end{algorithm}

% \textbf{Inputs:} Point set \(\mathcal{P}\)

\subsection{DiskANN}

DiskANN works in a similar way to HNSW but in a single layer. As shown in Figure \ref{fig:diskann-viz}, it constructs a graph with both short and long edges within a single layer and the connections spread out from the center of the graph which is the starting point.

The construction is rather simple. It is essentially Algorithm \ref{alg:build-index} with the medoid of the data set being the starting point. The pruning criteria ensures that the points in the path from the starting point to the new point are not to close to each other. The search is essentially Algorithm \ref{alg:greedy-search} starting from the medoid.

% \begin{algorithm}[H]
% \caption{RobustPrune Algorithm}\label{alg:robust-prune}
% \begin{algorithmic}[1]
%     \Procedure{RobustPrune}{\(G, p, \mathcal{V}, \alpha, M\){}}
%         \State{\(\mathcal{V} \gets \mathcal{V} \cup N_{\text{out}}(p) \setminus \left\{p\right\};N_\text{out}(p) \gets \emptyset\)}
%         \While{\(\mathcal{V} \neq \emptyset\)}
%             \State{\(p^* \gets \arg\min_{p \in \mathcal{V}} \delta(p, q)\)}
%             \State{\(N_\text{out}(p) \gets N_\text{out}(p) \cup \left\{p^*\right\}\)}
%             \If{\(|N_\text{out}(p) = M\)}
%                 \State{break}
%             \EndIf
%             \For{\(p' \in \mathcal{V}\)}
%                 \If{\(\alpha \cdot \delta(p^*, p') \leq \delta(p, p')\)}
%                     \State{\(\mathcal{V} \gets \mathcal{V} \setminus \left\{p'\right\}\)}
%                 \EndIf
%             \EndFor
%         \EndWhile
%     \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% \textbf{Inputs:} Graph \(G\), Starting point \(p\), Candidate set \(\mathcal{V}\), Distance scaler \(\alpha\), Max degree \(M\)

% \begin{algorithm}[H]
% \caption{Vamana Algorithm}\label{alg:vamana}
% \begin{algorithmic}[1]
%     \Function{Vamana}{\(\mathcal{P}, \alpha, C, M\){}}
%         \State{Initialize \(G\) as random directed graph with max degree \(M\)}
%         \State{\(s \gets \textsc{Medoid}(\mathcal{P})\); \(n \gets |\mathcal{P}|\)}
%         \For{\(p \in \mathcal{P}\)}
%             \State{\(\mathcal{L}, \mathcal{V} \gets \textsc{GreedySearch}(s, p, 1, C)\)}
%             \State{\(\textsc{RobustPrune}(p, \mathcal{V}, \alpha, M)\)}
%             \For{\(p' \in N_\text{out}(p)\)}
%                 \If{\(|N_\text{out}(p') \cup \left\{p'\right\}| > M\)}
%                     \State{\(\textsc{RobustPrune}(p', N_\text{out}(p')\cup\left\{p'\right\}, \alpha, M)\)}
%                 \Else
%                     \State{\(N_\text{out}(p') \gets N_\text{out}(p') \cup \left\{p'\right\}\)}
%                 \EndIf
%             \EndFor
%         \EndFor
%         \State{\Return{\(G\)}}
%     \EndFunction
% \end{algorithmic}
% \end{algorithm}

% \textbf{Inputs:} Point set \(\mathcal{P}\), Dist. scaler \(\alpha\), Candidate size \(C\), Max degree \(M\)

\subsection{Similarities and Differences}

A common pattern that shows up in both algorithms is the concept of long and short edges. In short, long edges allows for the algorithm to traverse from the starting point to the query point very quickly and the short edges allow for \textit{fine-tuning} the results.

However, the structure of the graph and its construction algorithm are slightly different. Namely, HNSW is a multi-layer undirected graph while DiskANN is a single-layer directed graph. Their pruning heuristics (line 4 in Algorithm \ref{alg:build-index}) are also different.
